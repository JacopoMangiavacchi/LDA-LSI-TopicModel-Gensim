{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Import packages and stopwords**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pattern in /home/nbuser/anaconda2_501/lib/python2.7/site-packages (3.6)\n",
      "Requirement already satisfied: lxml in /home/nbuser/anaconda2_501/lib/python2.7/site-packages (from pattern) (4.2.5)\n",
      "Requirement already satisfied: mysqlclient in /home/nbuser/anaconda2_501/lib/python2.7/site-packages (from pattern) (1.4.4)\n",
      "Requirement already satisfied: feedparser in /home/nbuser/anaconda2_501/lib/python2.7/site-packages (from pattern) (5.2.1)\n",
      "Requirement already satisfied: pdfminer in /home/nbuser/anaconda2_501/lib/python2.7/site-packages (from pattern) (20140328)\n",
      "Requirement already satisfied: python-docx in /home/nbuser/anaconda2_501/lib/python2.7/site-packages (from pattern) (0.8.10)\n",
      "Requirement already satisfied: future in /home/nbuser/anaconda2_501/lib/python2.7/site-packages (from pattern) (0.15.2)\n",
      "Requirement already satisfied: backports.csv in /home/nbuser/anaconda2_501/lib/python2.7/site-packages (from pattern) (1.0.7)\n",
      "Requirement already satisfied: nltk in /home/nbuser/anaconda2_501/lib/python2.7/site-packages (from pattern) (3.3)\n",
      "Requirement already satisfied: requests in /home/nbuser/anaconda2_501/lib/python2.7/site-packages (from pattern) (2.20.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/nbuser/anaconda2_501/lib/python2.7/site-packages (from pattern) (4.6.3)\n",
      "Requirement already satisfied: numpy in /home/nbuser/anaconda2_501/lib/python2.7/site-packages (from pattern) (1.16.1)\n",
      "Requirement already satisfied: scipy in /home/nbuser/anaconda2_501/lib/python2.7/site-packages (from pattern) (1.1.0)\n",
      "Requirement already satisfied: cherrypy in /home/nbuser/anaconda2_501/lib/python2.7/site-packages (from pattern) (17.4.2)\n",
      "Requirement already satisfied: six in /home/nbuser/anaconda2_501/lib/python2.7/site-packages (from nltk->pattern) (1.11.0)\n",
      "Requirement already satisfied: idna<2.8,>=2.5 in /home/nbuser/anaconda2_501/lib/python2.7/site-packages (from requests->pattern) (2.7)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/nbuser/anaconda2_501/lib/python2.7/site-packages (from requests->pattern) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /home/nbuser/anaconda2_501/lib/python2.7/site-packages (from requests->pattern) (1.23)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/nbuser/anaconda2_501/lib/python2.7/site-packages (from requests->pattern) (2018.10.15)\n",
      "Requirement already satisfied: more-itertools in /home/nbuser/anaconda2_501/lib/python2.7/site-packages (from cherrypy->pattern) (4.3.0)\n",
      "Requirement already satisfied: contextlib2 in /home/nbuser/anaconda2_501/lib/python2.7/site-packages (from cherrypy->pattern) (0.4.0)\n",
      "Requirement already satisfied: cheroot>=6.2.4 in /home/nbuser/anaconda2_501/lib/python2.7/site-packages (from cherrypy->pattern) (6.5.6)\n",
      "Requirement already satisfied: zc.lockfile in /home/nbuser/anaconda2_501/lib/python2.7/site-packages (from cherrypy->pattern) (2.0)\n",
      "Requirement already satisfied: portend>=2.1.1 in /home/nbuser/anaconda2_501/lib/python2.7/site-packages (from cherrypy->pattern) (2.5)\n",
      "Requirement already satisfied: backports.functools-lru-cache in /home/nbuser/anaconda2_501/lib/python2.7/site-packages (from cheroot>=6.2.4->cherrypy->pattern) (1.5)\n",
      "Requirement already satisfied: setuptools in /home/nbuser/anaconda2_501/lib/python2.7/site-packages (from zc.lockfile->cherrypy->pattern) (41.1.0)\n",
      "Requirement already satisfied: tempora>=1.8 in /home/nbuser/anaconda2_501/lib/python2.7/site-packages (from portend>=2.1.1->cherrypy->pattern) (1.14.1)\n",
      "Requirement already satisfied: jaraco.functools>=1.20 in /home/nbuser/anaconda2_501/lib/python2.7/site-packages (from tempora>=1.8->portend>=2.1.1->cherrypy->pattern) (2.0)\n",
      "Requirement already satisfied: pytz in /home/nbuser/anaconda2_501/lib/python2.7/site-packages (from tempora>=1.8->portend>=2.1.1->cherrypy->pattern) (2018.7)\n",
      "\u001b[33mYou are using pip version 19.2.2, however version 19.2.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: gensim in /home/nbuser/anaconda2_501/lib/python2.7/site-packages (3.8.0)\n",
      "Requirement already satisfied, skipping upgrade: smart-open>=1.7.0 in /home/nbuser/anaconda2_501/lib/python2.7/site-packages (from gensim) (1.8.4)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.5.0 in /home/nbuser/anaconda2_501/lib/python2.7/site-packages (from gensim) (1.11.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy<=1.16.1,>=1.11.3 in /home/nbuser/anaconda2_501/lib/python2.7/site-packages (from gensim) (1.16.1)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.18.1 in /home/nbuser/anaconda2_501/lib/python2.7/site-packages (from gensim) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: boto>=2.32 in /home/nbuser/anaconda2_501/lib/python2.7/site-packages (from smart-open>=1.7.0->gensim) (2.49.0)\n",
      "Requirement already satisfied, skipping upgrade: requests in /home/nbuser/anaconda2_501/lib/python2.7/site-packages (from smart-open>=1.7.0->gensim) (2.20.1)\n",
      "Requirement already satisfied, skipping upgrade: bz2file in /home/nbuser/anaconda2_501/lib/python2.7/site-packages (from smart-open>=1.7.0->gensim) (0.98)\n",
      "Requirement already satisfied, skipping upgrade: boto3 in /home/nbuser/anaconda2_501/lib/python2.7/site-packages (from smart-open>=1.7.0->gensim) (1.4.8)\n",
      "Requirement already satisfied, skipping upgrade: idna<2.8,>=2.5 in /home/nbuser/anaconda2_501/lib/python2.7/site-packages (from requests->smart-open>=1.7.0->gensim) (2.7)\n",
      "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /home/nbuser/anaconda2_501/lib/python2.7/site-packages (from requests->smart-open>=1.7.0->gensim) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: urllib3<1.25,>=1.21.1 in /home/nbuser/anaconda2_501/lib/python2.7/site-packages (from requests->smart-open>=1.7.0->gensim) (1.23)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /home/nbuser/anaconda2_501/lib/python2.7/site-packages (from requests->smart-open>=1.7.0->gensim) (2018.10.15)\n",
      "Requirement already satisfied, skipping upgrade: jmespath<1.0.0,>=0.7.1 in /home/nbuser/anaconda2_501/lib/python2.7/site-packages (from boto3->smart-open>=1.7.0->gensim) (0.9.4)\n",
      "Requirement already satisfied, skipping upgrade: botocore<1.9.0,>=1.8.0 in /home/nbuser/anaconda2_501/lib/python2.7/site-packages (from boto3->smart-open>=1.7.0->gensim) (1.8.50)\n",
      "Requirement already satisfied, skipping upgrade: s3transfer<0.2.0,>=0.1.10 in /home/nbuser/anaconda2_501/lib/python2.7/site-packages (from boto3->smart-open>=1.7.0->gensim) (0.1.13)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil<3.0.0,>=2.1 in /home/nbuser/anaconda2_501/lib/python2.7/site-packages (from botocore<1.9.0,>=1.8.0->boto3->smart-open>=1.7.0->gensim) (2.7.5)\n",
      "Requirement already satisfied, skipping upgrade: docutils>=0.10 in /home/nbuser/anaconda2_501/lib/python2.7/site-packages (from botocore<1.9.0,>=1.8.0->boto3->smart-open>=1.7.0->gensim) (0.14)\n",
      "Requirement already satisfied, skipping upgrade: futures<4.0.0,>=2.2.0; python_version == \"2.6\" or python_version == \"2.7\" in /home/nbuser/anaconda2_501/lib/python2.7/site-packages (from s3transfer<0.2.0,>=0.1.10->boto3->smart-open>=1.7.0->gensim) (3.2.0)\n",
      "\u001b[33mYou are using pip version 19.2.2, however version 19.2.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -U gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection u'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /home/nbuser/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /home/nbuser/nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /home/nbuser/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /home/nbuser/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /home/nbuser/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /home/nbuser/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /home/nbuser/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /home/nbuser/nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /home/nbuser/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /home/nbuser/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /home/nbuser/nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to /home/nbuser/nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /home/nbuser/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /home/nbuser/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /home/nbuser/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /home/nbuser/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     /home/nbuser/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /home/nbuser/nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /home/nbuser/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"popular\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection u'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /home/nbuser/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /home/nbuser/nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /home/nbuser/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /home/nbuser/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /home/nbuser/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /home/nbuser/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /home/nbuser/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /home/nbuser/nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /home/nbuser/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /home/nbuser/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /home/nbuser/nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to /home/nbuser/nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /home/nbuser/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /home/nbuser/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /home/nbuser/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /home/nbuser/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     /home/nbuser/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /home/nbuser/nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /home/nbuser/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"popular\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LdaModel, LdaMulticore\n",
    "import gensim.downloader as api\n",
    "from gensim.utils import simple_preprocess, lemmatize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s')\n",
    "logging.root.setLevel(level=logging.INFO)\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words = stop_words + ['com', 'edu', 'subject', 'lines', 'organization', 'would', 'article', 'could']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import the dataset and get the text and real topic of each news article**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = api.load(\"text8\")\n",
    "data = [d for d in dataset]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prepare Data (Remove stopwords and lemmatize)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'anarchism', u'originated', u'term', u'abuse', u'first']\n"
     ]
    }
   ],
   "source": [
    "data_processed = []\n",
    "\n",
    "for i, doc in enumerate(data[:100]):\n",
    "    doc_out = []\n",
    "    for wd in doc:\n",
    "        if wd not in stop_words:  # remove stopwords\n",
    "            lemmatized_word = lemmatize(wd, allowed_tags=re.compile('(NN|JJ|RB)'))  # lemmatize\n",
    "            if lemmatized_word:\n",
    "                doc_out = doc_out + [lemmatized_word[0].split(b'/')[0].decode('utf-8')]\n",
    "        else:\n",
    "            continue\n",
    "    data_processed.append(doc_out)\n",
    "\n",
    "# Print a small sample    \n",
    "print(data_processed[0][:5]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create the Inputs of LDA model: Dictionary and Corpus**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-08-27 18:36:25,238 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-08-27 18:36:27,146 : INFO : built Dictionary(40074 unique tokens: [u'fawn', u'homomorphism', u'schlegel', u'nunnery', u'parallelogram']...) from 100 documents (total 425549 corpus positions)\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora\n",
    "dct = corpora.Dictionary(data_processed)\n",
    "corpus = [dct.doc2bow(line) for line in data_processed]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train the LDA model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-08-27 18:37:07,264 : INFO : using asymmetric alpha [0.26219156, 0.19027454, 0.14931786, 0.12287004, 0.104381524, 0.090729296, 0.080235206]\n",
      "2019-08-27 18:37:07,286 : INFO : using symmetric eta at 0.142857142857\n",
      "2019-08-27 18:37:07,313 : INFO : using serial LDA version on this node\n",
      "2019-08-27 18:37:07,548 : INFO : running online LDA training, 7 topics, 10 passes over the supplied corpus of 100 documents, updating every 1000 documents, evaluating every ~0 documents, iterating 100x with a convergence threshold of 0.001000\n",
      "2019-08-27 18:37:07,770 : INFO : training LDA model using 1 processes\n",
      "2019-08-27 18:37:16,450 : INFO : PROGRESS: pass 0, dispatched chunk #0 = documents up to #100/100, outstanding queue size 1\n",
      "2019-08-27 18:37:22,653 : INFO : topic #6 (0.080): 0.001*\"also\" + 0.001*\"american\" + 0.001*\"state\" + 0.001*\"person\" + 0.001*\"time\" + 0.001*\"world\" + 0.001*\"year\" + 0.001*\"war\" + 0.001*\"many\" + 0.001*\"first\"\n",
      "2019-08-27 18:37:22,657 : INFO : topic #5 (0.091): 0.001*\"also\" + 0.001*\"first\" + 0.001*\"american\" + 0.001*\"state\" + 0.001*\"many\" + 0.000*\"year\" + 0.000*\"war\" + 0.000*\"time\" + 0.000*\"person\" + 0.000*\"new\"\n",
      "2019-08-27 18:37:22,659 : INFO : topic #2 (0.149): 0.001*\"also\" + 0.001*\"first\" + 0.001*\"american\" + 0.001*\"time\" + 0.001*\"year\" + 0.001*\"state\" + 0.001*\"many\" + 0.001*\"number\" + 0.001*\"new\" + 0.001*\"world\"\n",
      "2019-08-27 18:37:22,661 : INFO : topic #1 (0.190): 0.001*\"also\" + 0.001*\"state\" + 0.001*\"first\" + 0.001*\"american\" + 0.001*\"many\" + 0.000*\"war\" + 0.000*\"time\" + 0.000*\"year\" + 0.000*\"system\" + 0.000*\"new\"\n",
      "2019-08-27 18:37:22,663 : INFO : topic #0 (0.262): 0.001*\"also\" + 0.001*\"first\" + 0.000*\"state\" + 0.000*\"american\" + 0.000*\"time\" + 0.000*\"year\" + 0.000*\"book\" + 0.000*\"many\" + 0.000*\"person\" + 0.000*\"new\"\n",
      "2019-08-27 18:37:22,671 : INFO : topic diff=0.232065, rho=0.125000\n",
      "2019-08-27 18:37:22,674 : INFO : PROGRESS: pass 1, dispatched chunk #0 = documents up to #100/100, outstanding queue size 1\n",
      "2019-08-27 18:37:26,863 : INFO : topic #6 (0.080): 0.003*\"also\" + 0.002*\"state\" + 0.002*\"american\" + 0.002*\"time\" + 0.002*\"person\" + 0.002*\"many\" + 0.001*\"world\" + 0.001*\"first\" + 0.001*\"war\" + 0.001*\"history\"\n",
      "2019-08-27 18:37:26,865 : INFO : topic #5 (0.091): 0.001*\"also\" + 0.001*\"abortion\" + 0.001*\"first\" + 0.001*\"american\" + 0.001*\"state\" + 0.001*\"many\" + 0.000*\"year\" + 0.000*\"time\" + 0.000*\"war\" + 0.000*\"person\"\n",
      "2019-08-27 18:37:26,867 : INFO : topic #2 (0.149): 0.003*\"also\" + 0.003*\"american\" + 0.002*\"first\" + 0.002*\"state\" + 0.002*\"year\" + 0.002*\"time\" + 0.002*\"many\" + 0.002*\"new\" + 0.001*\"war\" + 0.001*\"world\"\n",
      "2019-08-27 18:37:26,874 : INFO : topic #1 (0.190): 0.001*\"also\" + 0.001*\"state\" + 0.001*\"first\" + 0.001*\"many\" + 0.001*\"american\" + 0.000*\"war\" + 0.000*\"time\" + 0.000*\"year\" + 0.000*\"ammonia\" + 0.000*\"name\"\n",
      "2019-08-27 18:37:26,885 : INFO : topic #0 (0.262): 0.001*\"also\" + 0.001*\"first\" + 0.000*\"state\" + 0.000*\"american\" + 0.000*\"time\" + 0.000*\"book\" + 0.000*\"year\" + 0.000*\"many\" + 0.000*\"person\" + 0.000*\"new\"\n",
      "2019-08-27 18:37:26,888 : INFO : topic diff=0.156270, rho=0.124035\n",
      "2019-08-27 18:37:26,894 : INFO : PROGRESS: pass 2, dispatched chunk #0 = documents up to #100/100, outstanding queue size 1\n",
      "2019-08-27 18:37:29,504 : INFO : topic #6 (0.080): 0.003*\"also\" + 0.002*\"state\" + 0.002*\"time\" + 0.002*\"american\" + 0.002*\"first\" + 0.002*\"person\" + 0.002*\"many\" + 0.002*\"world\" + 0.002*\"language\" + 0.002*\"history\"\n",
      "2019-08-27 18:37:29,507 : INFO : topic #5 (0.091): 0.001*\"also\" + 0.001*\"abortion\" + 0.001*\"first\" + 0.001*\"american\" + 0.001*\"state\" + 0.001*\"many\" + 0.000*\"year\" + 0.000*\"time\" + 0.000*\"war\" + 0.000*\"person\"\n",
      "2019-08-27 18:37:29,509 : INFO : topic #2 (0.149): 0.004*\"also\" + 0.003*\"american\" + 0.003*\"state\" + 0.003*\"first\" + 0.003*\"year\" + 0.002*\"time\" + 0.002*\"many\" + 0.002*\"new\" + 0.002*\"war\" + 0.002*\"person\"\n",
      "2019-08-27 18:37:29,511 : INFO : topic #1 (0.190): 0.001*\"also\" + 0.001*\"state\" + 0.001*\"first\" + 0.001*\"ammonia\" + 0.001*\"many\" + 0.001*\"american\" + 0.000*\"war\" + 0.000*\"time\" + 0.000*\"year\" + 0.000*\"name\"\n",
      "2019-08-27 18:37:29,515 : INFO : topic #0 (0.262): 0.001*\"also\" + 0.001*\"first\" + 0.000*\"state\" + 0.000*\"american\" + 0.000*\"time\" + 0.000*\"book\" + 0.000*\"year\" + 0.000*\"many\" + 0.000*\"person\" + 0.000*\"new\"\n",
      "2019-08-27 18:37:29,517 : INFO : topic diff=0.121900, rho=0.123091\n",
      "2019-08-27 18:37:30,043 : INFO : PROGRESS: pass 3, dispatched chunk #0 = documents up to #100/100, outstanding queue size 1\n",
      "2019-08-27 18:37:31,807 : INFO : topic #6 (0.080): 0.004*\"also\" + 0.002*\"time\" + 0.002*\"first\" + 0.002*\"state\" + 0.002*\"american\" + 0.002*\"many\" + 0.002*\"person\" + 0.002*\"world\" + 0.002*\"language\" + 0.002*\"apollo\"\n",
      "2019-08-27 18:37:31,812 : INFO : topic #5 (0.091): 0.001*\"also\" + 0.001*\"abortion\" + 0.001*\"first\" + 0.001*\"american\" + 0.001*\"state\" + 0.001*\"many\" + 0.000*\"year\" + 0.000*\"time\" + 0.000*\"war\" + 0.000*\"person\"\n",
      "2019-08-27 18:37:31,820 : INFO : topic #2 (0.149): 0.005*\"also\" + 0.004*\"american\" + 0.004*\"state\" + 0.003*\"first\" + 0.003*\"year\" + 0.003*\"time\" + 0.003*\"many\" + 0.002*\"new\" + 0.002*\"war\" + 0.002*\"person\"\n",
      "2019-08-27 18:37:31,833 : INFO : topic #1 (0.190): 0.001*\"also\" + 0.001*\"state\" + 0.001*\"ammonia\" + 0.001*\"first\" + 0.001*\"many\" + 0.001*\"american\" + 0.000*\"war\" + 0.000*\"time\" + 0.000*\"year\" + 0.000*\"name\"\n",
      "2019-08-27 18:37:31,847 : INFO : topic #0 (0.262): 0.001*\"also\" + 0.001*\"first\" + 0.000*\"state\" + 0.000*\"american\" + 0.000*\"time\" + 0.000*\"book\" + 0.000*\"year\" + 0.000*\"many\" + 0.000*\"person\" + 0.000*\"new\"\n",
      "2019-08-27 18:37:31,850 : INFO : topic diff=0.112116, rho=0.122169\n",
      "2019-08-27 18:37:32,342 : INFO : PROGRESS: pass 4, dispatched chunk #0 = documents up to #100/100, outstanding queue size 1\n",
      "2019-08-27 18:37:33,945 : INFO : topic #6 (0.080): 0.004*\"also\" + 0.003*\"first\" + 0.003*\"time\" + 0.003*\"state\" + 0.002*\"many\" + 0.002*\"american\" + 0.002*\"person\" + 0.002*\"world\" + 0.002*\"language\" + 0.002*\"apollo\"\n",
      "2019-08-27 18:37:33,950 : INFO : topic #5 (0.091): 0.001*\"also\" + 0.001*\"abortion\" + 0.001*\"first\" + 0.001*\"american\" + 0.001*\"state\" + 0.000*\"many\" + 0.000*\"year\" + 0.000*\"time\" + 0.000*\"war\" + 0.000*\"person\"\n",
      "2019-08-27 18:37:33,954 : INFO : topic #2 (0.149): 0.005*\"also\" + 0.004*\"american\" + 0.004*\"state\" + 0.003*\"first\" + 0.003*\"year\" + 0.003*\"many\" + 0.003*\"time\" + 0.002*\"new\" + 0.002*\"war\" + 0.002*\"person\"\n",
      "2019-08-27 18:37:33,959 : INFO : topic #1 (0.190): 0.001*\"also\" + 0.001*\"state\" + 0.001*\"ammonia\" + 0.001*\"first\" + 0.001*\"many\" + 0.001*\"american\" + 0.000*\"war\" + 0.000*\"time\" + 0.000*\"year\" + 0.000*\"name\"\n",
      "2019-08-27 18:37:33,964 : INFO : topic #0 (0.262): 0.001*\"also\" + 0.001*\"first\" + 0.000*\"state\" + 0.000*\"american\" + 0.000*\"time\" + 0.000*\"book\" + 0.000*\"year\" + 0.000*\"many\" + 0.000*\"person\" + 0.000*\"new\"\n",
      "2019-08-27 18:37:33,967 : INFO : topic diff=0.110430, rho=0.121268\n",
      "2019-08-27 18:37:33,969 : INFO : PROGRESS: pass 5, dispatched chunk #0 = documents up to #100/100, outstanding queue size 1\n",
      "2019-08-27 18:37:35,726 : INFO : topic #6 (0.080): 0.004*\"also\" + 0.003*\"first\" + 0.003*\"time\" + 0.003*\"state\" + 0.003*\"many\" + 0.003*\"american\" + 0.003*\"person\" + 0.002*\"world\" + 0.002*\"apollo\" + 0.002*\"language\"\n",
      "2019-08-27 18:37:35,732 : INFO : topic #5 (0.091): 0.001*\"also\" + 0.001*\"abortion\" + 0.001*\"first\" + 0.001*\"american\" + 0.000*\"state\" + 0.000*\"many\" + 0.000*\"year\" + 0.000*\"time\" + 0.000*\"war\" + 0.000*\"person\"\n",
      "2019-08-27 18:37:35,735 : INFO : topic #2 (0.149): 0.005*\"also\" + 0.004*\"american\" + 0.004*\"state\" + 0.004*\"first\" + 0.003*\"year\" + 0.003*\"many\" + 0.003*\"time\" + 0.002*\"new\" + 0.002*\"war\" + 0.002*\"person\"\n",
      "2019-08-27 18:37:35,740 : INFO : topic #1 (0.190): 0.001*\"also\" + 0.001*\"state\" + 0.001*\"ammonia\" + 0.001*\"first\" + 0.000*\"many\" + 0.000*\"american\" + 0.000*\"war\" + 0.000*\"time\" + 0.000*\"year\" + 0.000*\"name\"\n",
      "2019-08-27 18:37:35,746 : INFO : topic #0 (0.262): 0.001*\"also\" + 0.001*\"first\" + 0.000*\"state\" + 0.000*\"american\" + 0.000*\"time\" + 0.000*\"book\" + 0.000*\"year\" + 0.000*\"many\" + 0.000*\"person\" + 0.000*\"new\"\n",
      "2019-08-27 18:37:35,749 : INFO : topic diff=0.111988, rho=0.120386\n",
      "2019-08-27 18:37:35,753 : INFO : PROGRESS: pass 6, dispatched chunk #0 = documents up to #100/100, outstanding queue size 1\n",
      "2019-08-27 18:37:38,006 : INFO : topic #6 (0.080): 0.005*\"also\" + 0.003*\"first\" + 0.003*\"time\" + 0.003*\"many\" + 0.003*\"state\" + 0.003*\"american\" + 0.003*\"person\" + 0.003*\"world\" + 0.002*\"apollo\" + 0.002*\"language\"\n",
      "2019-08-27 18:37:38,013 : INFO : topic #5 (0.091): 0.001*\"also\" + 0.001*\"abortion\" + 0.001*\"first\" + 0.001*\"american\" + 0.000*\"state\" + 0.000*\"many\" + 0.000*\"year\" + 0.000*\"time\" + 0.000*\"war\" + 0.000*\"person\"\n",
      "2019-08-27 18:37:38,015 : INFO : topic #2 (0.149): 0.005*\"also\" + 0.004*\"american\" + 0.004*\"state\" + 0.004*\"first\" + 0.003*\"year\" + 0.003*\"many\" + 0.003*\"time\" + 0.003*\"new\" + 0.002*\"war\" + 0.002*\"person\"\n",
      "2019-08-27 18:37:38,021 : INFO : topic #1 (0.190): 0.001*\"also\" + 0.001*\"state\" + 0.001*\"ammonia\" + 0.001*\"first\" + 0.000*\"many\" + 0.000*\"american\" + 0.000*\"war\" + 0.000*\"time\" + 0.000*\"year\" + 0.000*\"name\"\n",
      "2019-08-27 18:37:38,029 : INFO : topic #0 (0.262): 0.001*\"also\" + 0.001*\"first\" + 0.000*\"state\" + 0.000*\"american\" + 0.000*\"time\" + 0.000*\"book\" + 0.000*\"year\" + 0.000*\"many\" + 0.000*\"person\" + 0.000*\"new\"\n",
      "2019-08-27 18:37:38,033 : INFO : topic diff=0.115116, rho=0.119523\n",
      "2019-08-27 18:37:38,522 : INFO : PROGRESS: pass 7, dispatched chunk #0 = documents up to #100/100, outstanding queue size 1\n",
      "2019-08-27 18:37:40,187 : INFO : topic #6 (0.080): 0.005*\"also\" + 0.003*\"first\" + 0.003*\"time\" + 0.003*\"many\" + 0.003*\"state\" + 0.003*\"american\" + 0.003*\"person\" + 0.003*\"world\" + 0.003*\"apollo\" + 0.003*\"language\"\n",
      "2019-08-27 18:37:40,189 : INFO : topic #5 (0.091): 0.001*\"also\" + 0.001*\"abortion\" + 0.001*\"first\" + 0.001*\"american\" + 0.000*\"state\" + 0.000*\"many\" + 0.000*\"year\" + 0.000*\"time\" + 0.000*\"war\" + 0.000*\"person\"\n",
      "2019-08-27 18:37:40,195 : INFO : topic #2 (0.149): 0.005*\"also\" + 0.004*\"american\" + 0.004*\"state\" + 0.004*\"first\" + 0.003*\"year\" + 0.003*\"many\" + 0.003*\"time\" + 0.003*\"new\" + 0.003*\"war\" + 0.002*\"person\"\n",
      "2019-08-27 18:37:40,198 : INFO : topic #1 (0.190): 0.001*\"also\" + 0.001*\"state\" + 0.001*\"ammonia\" + 0.001*\"first\" + 0.000*\"many\" + 0.000*\"american\" + 0.000*\"war\" + 0.000*\"time\" + 0.000*\"year\" + 0.000*\"name\"\n",
      "2019-08-27 18:37:40,201 : INFO : topic #0 (0.262): 0.001*\"also\" + 0.000*\"first\" + 0.000*\"state\" + 0.000*\"american\" + 0.000*\"time\" + 0.000*\"book\" + 0.000*\"year\" + 0.000*\"many\" + 0.000*\"person\" + 0.000*\"new\"\n",
      "2019-08-27 18:37:40,204 : INFO : topic diff=0.119051, rho=0.118678\n",
      "2019-08-27 18:37:40,598 : INFO : PROGRESS: pass 8, dispatched chunk #0 = documents up to #100/100, outstanding queue size 1\n",
      "2019-08-27 18:37:42,083 : INFO : topic #6 (0.080): 0.005*\"also\" + 0.004*\"first\" + 0.003*\"time\" + 0.003*\"many\" + 0.003*\"state\" + 0.003*\"american\" + 0.003*\"world\" + 0.003*\"person\" + 0.003*\"apollo\" + 0.003*\"language\"\n",
      "2019-08-27 18:37:42,085 : INFO : topic #5 (0.091): 0.001*\"also\" + 0.001*\"abortion\" + 0.001*\"first\" + 0.001*\"american\" + 0.000*\"state\" + 0.000*\"many\" + 0.000*\"year\" + 0.000*\"time\" + 0.000*\"war\" + 0.000*\"person\"\n",
      "2019-08-27 18:37:42,090 : INFO : topic #2 (0.149): 0.005*\"also\" + 0.004*\"american\" + 0.004*\"state\" + 0.004*\"first\" + 0.003*\"year\" + 0.003*\"many\" + 0.003*\"time\" + 0.003*\"new\" + 0.003*\"war\" + 0.003*\"person\"\n",
      "2019-08-27 18:37:42,093 : INFO : topic #1 (0.190): 0.001*\"also\" + 0.001*\"state\" + 0.001*\"ammonia\" + 0.001*\"first\" + 0.000*\"many\" + 0.000*\"american\" + 0.000*\"war\" + 0.000*\"time\" + 0.000*\"year\" + 0.000*\"name\"\n",
      "2019-08-27 18:37:42,095 : INFO : topic #0 (0.262): 0.001*\"also\" + 0.000*\"first\" + 0.000*\"state\" + 0.000*\"american\" + 0.000*\"time\" + 0.000*\"book\" + 0.000*\"year\" + 0.000*\"many\" + 0.000*\"person\" + 0.000*\"new\"\n",
      "2019-08-27 18:37:42,097 : INFO : topic diff=0.123347, rho=0.117851\n",
      "2019-08-27 18:37:42,099 : INFO : PROGRESS: pass 9, dispatched chunk #0 = documents up to #100/100, outstanding queue size 1\n",
      "2019-08-27 18:37:44,246 : INFO : topic #6 (0.080): 0.005*\"also\" + 0.004*\"first\" + 0.003*\"time\" + 0.003*\"many\" + 0.003*\"state\" + 0.003*\"world\" + 0.003*\"american\" + 0.003*\"person\" + 0.003*\"apollo\" + 0.003*\"language\"\n",
      "2019-08-27 18:37:44,248 : INFO : topic #5 (0.091): 0.001*\"also\" + 0.001*\"abortion\" + 0.001*\"first\" + 0.001*\"american\" + 0.000*\"state\" + 0.000*\"many\" + 0.000*\"year\" + 0.000*\"time\" + 0.000*\"war\" + 0.000*\"person\"\n",
      "2019-08-27 18:37:44,250 : INFO : topic #2 (0.149): 0.005*\"also\" + 0.004*\"american\" + 0.004*\"state\" + 0.004*\"first\" + 0.003*\"year\" + 0.003*\"many\" + 0.003*\"time\" + 0.003*\"new\" + 0.003*\"war\" + 0.003*\"person\"\n",
      "2019-08-27 18:37:44,252 : INFO : topic #1 (0.190): 0.001*\"also\" + 0.001*\"state\" + 0.001*\"ammonia\" + 0.000*\"first\" + 0.000*\"many\" + 0.000*\"american\" + 0.000*\"war\" + 0.000*\"time\" + 0.000*\"year\" + 0.000*\"name\"\n",
      "2019-08-27 18:37:44,254 : INFO : topic #0 (0.262): 0.001*\"also\" + 0.000*\"first\" + 0.000*\"state\" + 0.000*\"american\" + 0.000*\"time\" + 0.000*\"book\" + 0.000*\"year\" + 0.000*\"many\" + 0.000*\"person\" + 0.000*\"new\"\n",
      "2019-08-27 18:37:44,256 : INFO : topic diff=0.127679, rho=0.117041\n",
      "2019-08-27 18:37:44,372 : INFO : saving LdaState object under lda_model.model.state, separately None\n",
      "2019-08-27 18:37:45,117 : INFO : saved lda_model.model.state\n",
      "2019-08-27 18:37:45,859 : INFO : saving LdaMulticore object under lda_model.model, separately ['expElogbeta', 'sstats']\n",
      "2019-08-27 18:37:45,861 : INFO : not storing attribute id2word\n",
      "2019-08-27 18:37:45,862 : INFO : storing np array 'expElogbeta' to lda_model.model.expElogbeta.npy\n",
      "2019-08-27 18:37:46,188 : INFO : not storing attribute state\n",
      "2019-08-27 18:37:46,189 : INFO : not storing attribute dispatcher\n",
      "2019-08-27 18:37:46,289 : INFO : saved lda_model.model\n",
      "2019-08-27 18:37:46,294 : INFO : topic #0 (0.262): 0.001*\"also\" + 0.000*\"first\" + 0.000*\"state\" + 0.000*\"american\" + 0.000*\"time\" + 0.000*\"book\" + 0.000*\"year\" + 0.000*\"many\" + 0.000*\"person\" + 0.000*\"new\"\n",
      "2019-08-27 18:37:46,299 : INFO : topic #1 (0.190): 0.001*\"also\" + 0.001*\"state\" + 0.001*\"ammonia\" + 0.000*\"first\" + 0.000*\"many\" + 0.000*\"american\" + 0.000*\"war\" + 0.000*\"time\" + 0.000*\"year\" + 0.000*\"name\"\n",
      "2019-08-27 18:37:46,301 : INFO : topic #2 (0.149): 0.005*\"also\" + 0.004*\"american\" + 0.004*\"state\" + 0.004*\"first\" + 0.003*\"year\" + 0.003*\"many\" + 0.003*\"time\" + 0.003*\"new\" + 0.003*\"war\" + 0.003*\"person\"\n",
      "2019-08-27 18:37:46,303 : INFO : topic #3 (0.123): 0.001*\"atheism\" + 0.001*\"also\" + 0.001*\"first\" + 0.000*\"atheist\" + 0.000*\"american\" + 0.000*\"god\" + 0.000*\"state\" + 0.000*\"many\" + 0.000*\"new\" + 0.000*\"year\"\n",
      "2019-08-27 18:37:46,305 : INFO : topic #4 (0.104): 0.001*\"state\" + 0.001*\"also\" + 0.001*\"many\" + 0.000*\"world\" + 0.000*\"agave\" + 0.000*\"time\" + 0.000*\"new\" + 0.000*\"war\" + 0.000*\"god\" + 0.000*\"person\"\n",
      "2019-08-27 18:37:46,307 : INFO : topic #5 (0.091): 0.001*\"also\" + 0.001*\"abortion\" + 0.001*\"first\" + 0.001*\"american\" + 0.000*\"state\" + 0.000*\"many\" + 0.000*\"year\" + 0.000*\"time\" + 0.000*\"war\" + 0.000*\"person\"\n",
      "2019-08-27 18:37:46,309 : INFO : topic #6 (0.080): 0.005*\"also\" + 0.004*\"first\" + 0.003*\"time\" + 0.003*\"many\" + 0.003*\"state\" + 0.003*\"world\" + 0.003*\"american\" + 0.003*\"person\" + 0.003*\"apollo\" + 0.003*\"language\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  u'0.001*\"also\" + 0.000*\"first\" + 0.000*\"state\" + 0.000*\"american\" + 0.000*\"time\" + 0.000*\"book\" + 0.000*\"year\" + 0.000*\"many\" + 0.000*\"person\" + 0.000*\"new\"'),\n",
       " (1,\n",
       "  u'0.001*\"also\" + 0.001*\"state\" + 0.001*\"ammonia\" + 0.000*\"first\" + 0.000*\"many\" + 0.000*\"american\" + 0.000*\"war\" + 0.000*\"time\" + 0.000*\"year\" + 0.000*\"name\"'),\n",
       " (2,\n",
       "  u'0.005*\"also\" + 0.004*\"american\" + 0.004*\"state\" + 0.004*\"first\" + 0.003*\"year\" + 0.003*\"many\" + 0.003*\"time\" + 0.003*\"new\" + 0.003*\"war\" + 0.003*\"person\"'),\n",
       " (3,\n",
       "  u'0.001*\"atheism\" + 0.001*\"also\" + 0.001*\"first\" + 0.000*\"atheist\" + 0.000*\"american\" + 0.000*\"god\" + 0.000*\"state\" + 0.000*\"many\" + 0.000*\"new\" + 0.000*\"year\"'),\n",
       " (4,\n",
       "  u'0.001*\"state\" + 0.001*\"also\" + 0.001*\"many\" + 0.000*\"world\" + 0.000*\"agave\" + 0.000*\"time\" + 0.000*\"new\" + 0.000*\"war\" + 0.000*\"god\" + 0.000*\"person\"'),\n",
       " (5,\n",
       "  u'0.001*\"also\" + 0.001*\"abortion\" + 0.001*\"first\" + 0.001*\"american\" + 0.000*\"state\" + 0.000*\"many\" + 0.000*\"year\" + 0.000*\"time\" + 0.000*\"war\" + 0.000*\"person\"'),\n",
       " (6,\n",
       "  u'0.005*\"also\" + 0.004*\"first\" + 0.003*\"time\" + 0.003*\"many\" + 0.003*\"state\" + 0.003*\"world\" + 0.003*\"american\" + 0.003*\"person\" + 0.003*\"apollo\" + 0.003*\"language\"')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model = LdaMulticore(corpus=corpus,\n",
    "                         id2word=dct,\n",
    "                         random_state=100,\n",
    "                         num_topics=7,\n",
    "                         passes=10,\n",
    "                         chunksize=1000,\n",
    "                         batch=False,\n",
    "                         alpha='asymmetric',\n",
    "                         decay=0.5,\n",
    "                         offset=64,\n",
    "                         eta=None,\n",
    "                         eval_every=0,\n",
    "                         iterations=100,\n",
    "                         gamma_threshold=0.001,\n",
    "                         per_word_topics=True)\n",
    "\n",
    "# save the model\n",
    "lda_model.save('lda_model.model')\n",
    "\n",
    "# See the topics\n",
    "lda_model.print_topics(-1)\n",
    "# [(0, '0.001*\"also\" + 0.000*\"first\" + 0.000*\"state\" + 0.000*\"american\" + 0.000*\"time\" + 0.000*\"book\" + 0.000*\"year\" + 0.000*\"many\" + 0.000*\"person\" + 0.000*\"new\"'),\n",
    "#  (1, '0.001*\"also\" + 0.001*\"state\" + 0.001*\"ammonia\" + 0.000*\"first\" + 0.000*\"many\" + 0.000*\"american\" + 0.000*\"war\" + 0.000*\"time\" + 0.000*\"year\" + 0.000*\"name\"'),\n",
    "#  (2, '0.005*\"also\" + 0.004*\"american\" + 0.004*\"state\" + 0.004*\"first\" + 0.003*\"year\" + 0.003*\"many\" + 0.003*\"time\" + 0.003*\"new\" + 0.003*\"war\" + 0.003*\"person\"'),\n",
    "#  (3, '0.001*\"atheism\" + 0.001*\"also\" + 0.001*\"first\" + 0.001*\"atheist\" + 0.001*\"american\" + 0.000*\"god\" + 0.000*\"state\" + 0.000*\"many\" + 0.000*\"new\" + 0.000*\"year\"'),\n",
    "#  (4, '0.001*\"state\" + 0.001*\"also\" + 0.001*\"many\" + 0.000*\"world\" + 0.000*\"agave\" + 0.000*\"time\" + 0.000*\"new\" + 0.000*\"war\" + 0.000*\"god\" + 0.000*\"person\"'),\n",
    "#  (5, '0.001*\"also\" + 0.001*\"abortion\" + 0.001*\"first\" + 0.001*\"american\" + 0.000*\"state\" + 0.000*\"many\" + 0.000*\"year\" + 0.000*\"time\" + 0.000*\"war\" + 0.000*\"person\"'),\n",
    "#  (6, '0.005*\"also\" + 0.004*\"first\" + 0.003*\"time\" + 0.003*\"many\" + 0.003*\"state\" + 0.003*\"world\" + 0.003*\"american\" + 0.003*\"person\" + 0.003*\"apollo\" + 0.003*\"language\"')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpret the LDA Topic Modelâ€™s output**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Document Topics      : ', [(2, 0.96452165), (6, 0.035288557)])\n",
      "('Word id, Topics      : ', [(0, [2, 6]), (7, [2, 6]), (10, [2, 6])])\n",
      "('Phi Values (word id) : ', [(0, [(2, 2.8961294), (6, 0.10216269)]), (7, [(2, 0.86925656), (6, 0.08887509)])])\n",
      "('Word, Topics         : ', [(u'ability', [2, 6]), (u'absurdity', [2, 6])])\n",
      "('Phi Values (word)    : ', [(u'ability', [(2, 2.8961294), (6, 0.10216269)]), (u'absurdity', [(2, 0.86925656), (6, 0.08887509)])])\n",
      "------------------------------------------------------\n",
      "\n",
      "('Document Topics      : ', [(6, 0.99933493)])\n",
      "('Word id, Topics      : ', [(0, [6]), (10, [6]), (16, [6])])\n",
      "('Phi Values (word id) : ', [(0, [(6, 5.994235)]), (10, [(6, 2.9965134)])])\n",
      "('Word, Topics         : ', [(u'ability', [6]), (u'academic', [6])])\n",
      "('Phi Values (word)    : ', [(u'ability', [(6, 5.994235)]), (u'academic', [(6, 2.9965134)])])\n",
      "------------------------------------------------------\n",
      "\n",
      "('Document Topics      : ', [(6, 0.9998019)])\n",
      "('Word id, Topics      : ', [(1, [6]), (10, [6]), (15, [6])])\n",
      "('Phi Values (word id) : ', [(1, [(6, 0.99959904)]), (10, [(6, 5.9945807)])])\n",
      "('Word, Topics         : ', [(u'able', [6]), (u'academic', [6])])\n",
      "('Phi Values (word)    : ', [(u'able', [(6, 0.99959904)]), (u'academic', [(6, 5.9945807)])])\n",
      "------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reference: https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/topic_methods.ipynb\n",
    "for c in lda_model[corpus[5:8]]:\n",
    "    print(\"Document Topics      : \", c[0])      # [(Topics, Perc Contrib)]\n",
    "    print(\"Word id, Topics      : \", c[1][:3])  # [(Word id, [Topics])]\n",
    "    print(\"Phi Values (word id) : \", c[2][:2])  # [(Word id, [(Topic, Phi Value)])]\n",
    "    print(\"Word, Topics         : \", [(dct[wd], topic) for wd, topic in c[1][:2]])   # [(Word, [Topics])]\n",
    "    print(\"Phi Values (word)    : \", [(dct[wd], topic) for wd, topic in c[2][:2]])  # [(Word, [(Topic, Phi Value)])]\n",
    "    print(\"------------------------------------------------------\\n\")\n",
    "\n",
    "#> Document Topics      :  [(2, 0.96124125), (6, 0.038569752)]\n",
    "#> Word id, Topics      :  [(0, [2, 6]), (7, [2, 6]), (10, [2, 6])]\n",
    "#> Phi Values (word id) :  [(0, [(2, 2.887749), (6, 0.112249866)]), (7, [(2, 0.90105206), (6, 0.09893738)])]\n",
    "#> Word, Topics         :  [('ability', [2, 6]), ('absurdity', [2, 6])]\n",
    "#> Phi Values (word)    :  [('ability', [(2, 2.887749), (6, 0.112249866)]), ('absurdity', [(2, 0.90105206), (6, 0.09893738)])]\n",
    "#> ------------------------------------------------------\n",
    "\n",
    "#> Document Topics      :  [(6, 0.9997751)]\n",
    "#> Word id, Topics      :  [(0, [6]), (10, [6]), (16, [6])]\n",
    "#> Phi Values (word id) :  [(0, [(6, 5.9999967)]), (10, [(6, 2.9999983)])]\n",
    "#> Word, Topics         :  [('ability', [6]), ('academic', [6])]\n",
    "#> Phi Values (word)    :  [('ability', [(6, 5.9999967)]), ('academic', [(6, 2.9999983)])]\n",
    "#> ------------------------------------------------------\n",
    "\n",
    "#> Document Topics      :  [(6, 0.9998023)]\n",
    "#> Word id, Topics      :  [(1, [6]), (10, [6]), (15, [6])]\n",
    "#> Phi Values (word id) :  [(1, [(6, 0.99999917)]), (10, [(6, 5.999997)])]\n",
    "#> Word, Topics         :  [('able', [6]), ('academic', [6])]\n",
    "#> Phi Values (word)    :  [('able', [(6, 0.99999917)]), ('academic', [(6, 5.999997)])]\n",
    "#> ------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a LSI topic model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-08-27 18:39:49,585 : INFO : using serial LSI version on this node\n",
      "2019-08-27 18:39:49,588 : INFO : updating model with new documents\n",
      "2019-08-27 18:39:49,591 : INFO : preparing a new chunk of documents\n",
      "2019-08-27 18:39:49,690 : INFO : using 100 extra samples and 2 power iterations\n",
      "2019-08-27 18:39:49,693 : INFO : 1st phase: constructing (40074, 107) action matrix\n",
      "2019-08-27 18:39:49,739 : INFO : orthonormalizing (40074, 107) action matrix\n",
      "2019-08-27 18:39:50,750 : INFO : 2nd phase: running dense svd on (107, 100) matrix\n",
      "2019-08-27 18:39:50,797 : INFO : computing the final decomposition\n",
      "2019-08-27 18:39:50,798 : INFO : keeping 7 factors (discarding 62.871% of energy spectrum)\n",
      "2019-08-27 18:39:50,844 : INFO : processed documents up to #100\n",
      "2019-08-27 18:39:50,850 : INFO : topic #0(973.794): 0.262*\"also\" + 0.197*\"state\" + 0.197*\"american\" + 0.178*\"first\" + 0.151*\"many\" + 0.149*\"time\" + 0.147*\"year\" + 0.130*\"person\" + 0.130*\"world\" + 0.124*\"war\"\n",
      "2019-08-27 18:39:50,852 : INFO : topic #1(572.318): 0.937*\"agave\" + 0.164*\"asia\" + 0.100*\"aruba\" + 0.063*\"plant\" + 0.053*\"var\" + 0.052*\"state\" + 0.045*\"east\" + 0.044*\"congress\" + -0.042*\"first\" + 0.041*\"maguey\"\n",
      "2019-08-27 18:39:50,868 : INFO : topic #2(401.785): 0.507*\"american\" + 0.180*\"football\" + 0.179*\"player\" + 0.168*\"war\" + 0.150*\"british\" + -0.140*\"also\" + 0.114*\"ball\" + 0.110*\"day\" + -0.107*\"atheism\" + -0.106*\"god\"\n",
      "2019-08-27 18:39:50,871 : INFO : topic #3(334.148): -0.362*\"apollo\" + 0.248*\"lincoln\" + 0.211*\"state\" + -0.172*\"player\" + -0.151*\"football\" + 0.127*\"union\" + -0.125*\"ball\" + 0.124*\"government\" + -0.116*\"moon\" + 0.116*\"jews\"\n",
      "2019-08-27 18:39:50,880 : INFO : topic #4(322.189): -0.363*\"atheism\" + -0.334*\"god\" + -0.329*\"lincoln\" + -0.230*\"apollo\" + -0.215*\"atheist\" + -0.143*\"abraham\" + 0.136*\"island\" + -0.132*\"aristotle\" + 0.124*\"aluminium\" + -0.119*\"belief\"\n",
      "2019-08-27 18:39:50,886 : INFO : topic #0(973.794): 0.262*\"also\" + 0.197*\"state\" + 0.197*\"american\" + 0.178*\"first\" + 0.151*\"many\" + 0.149*\"time\" + 0.147*\"year\" + 0.130*\"person\" + 0.130*\"world\" + 0.124*\"war\"\n",
      "2019-08-27 18:39:50,897 : INFO : topic #1(572.318): 0.937*\"agave\" + 0.164*\"asia\" + 0.100*\"aruba\" + 0.063*\"plant\" + 0.053*\"var\" + 0.052*\"state\" + 0.045*\"east\" + 0.044*\"congress\" + -0.042*\"first\" + 0.041*\"maguey\"\n",
      "2019-08-27 18:39:50,902 : INFO : topic #2(401.785): 0.507*\"american\" + 0.180*\"football\" + 0.179*\"player\" + 0.168*\"war\" + 0.150*\"british\" + -0.140*\"also\" + 0.114*\"ball\" + 0.110*\"day\" + -0.107*\"atheism\" + -0.106*\"god\"\n",
      "2019-08-27 18:39:50,916 : INFO : topic #3(334.148): -0.362*\"apollo\" + 0.248*\"lincoln\" + 0.211*\"state\" + -0.172*\"player\" + -0.151*\"football\" + 0.127*\"union\" + -0.125*\"ball\" + 0.124*\"government\" + -0.116*\"moon\" + 0.116*\"jews\"\n",
      "2019-08-27 18:39:50,921 : INFO : topic #4(322.189): -0.363*\"atheism\" + -0.334*\"god\" + -0.329*\"lincoln\" + -0.230*\"apollo\" + -0.215*\"atheist\" + -0.143*\"abraham\" + 0.136*\"island\" + -0.132*\"aristotle\" + 0.124*\"aluminium\" + -0.119*\"belief\"\n",
      "2019-08-27 18:39:50,924 : INFO : topic #5(315.315): -0.360*\"apollo\" + 0.344*\"atheism\" + -0.326*\"lincoln\" + 0.226*\"god\" + 0.205*\"atheist\" + 0.139*\"american\" + -0.130*\"lunar\" + 0.128*\"football\" + -0.125*\"moon\" + 0.114*\"belief\"\n",
      "2019-08-27 18:39:50,928 : INFO : topic #6(312.092): -0.313*\"lincoln\" + 0.226*\"apollo\" + -0.166*\"football\" + -0.163*\"war\" + 0.162*\"god\" + 0.153*\"australia\" + -0.148*\"play\" + -0.146*\"ball\" + 0.122*\"atheism\" + -0.122*\"line\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, u'0.262*\"also\" + 0.197*\"state\" + 0.197*\"american\" + 0.178*\"first\" + 0.151*\"many\" + 0.149*\"time\" + 0.147*\"year\" + 0.130*\"person\" + 0.130*\"world\" + 0.124*\"war\"'), (1, u'0.937*\"agave\" + 0.164*\"asia\" + 0.100*\"aruba\" + 0.063*\"plant\" + 0.053*\"var\" + 0.052*\"state\" + 0.045*\"east\" + 0.044*\"congress\" + -0.042*\"first\" + 0.041*\"maguey\"'), (2, u'0.507*\"american\" + 0.180*\"football\" + 0.179*\"player\" + 0.168*\"war\" + 0.150*\"british\" + -0.140*\"also\" + 0.114*\"ball\" + 0.110*\"day\" + -0.107*\"atheism\" + -0.106*\"god\"'), (3, u'-0.362*\"apollo\" + 0.248*\"lincoln\" + 0.211*\"state\" + -0.172*\"player\" + -0.151*\"football\" + 0.127*\"union\" + -0.125*\"ball\" + 0.124*\"government\" + -0.116*\"moon\" + 0.116*\"jews\"'), (4, u'-0.363*\"atheism\" + -0.334*\"god\" + -0.329*\"lincoln\" + -0.230*\"apollo\" + -0.215*\"atheist\" + -0.143*\"abraham\" + 0.136*\"island\" + -0.132*\"aristotle\" + 0.124*\"aluminium\" + -0.119*\"belief\"'), (5, u'-0.360*\"apollo\" + 0.344*\"atheism\" + -0.326*\"lincoln\" + 0.226*\"god\" + 0.205*\"atheist\" + 0.139*\"american\" + -0.130*\"lunar\" + 0.128*\"football\" + -0.125*\"moon\" + 0.114*\"belief\"'), (6, u'-0.313*\"lincoln\" + 0.226*\"apollo\" + -0.166*\"football\" + -0.163*\"war\" + 0.162*\"god\" + 0.153*\"australia\" + -0.148*\"play\" + -0.146*\"ball\" + 0.122*\"atheism\" + -0.122*\"line\"')]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import LsiModel\n",
    "\n",
    "# Build the LSI Model\n",
    "lsi_model = LsiModel(corpus=corpus, id2word=dct, num_topics=7, decay=0.5)\n",
    "\n",
    "# View Topics\n",
    "print(lsi_model.print_topics(-1))\n",
    "#> [(0, '0.262*\"also\" + 0.197*\"state\" + 0.197*\"american\" + 0.178*\"first\" + '\n",
    "#>   '0.151*\"many\" + 0.149*\"time\" + 0.147*\"year\" + 0.130*\"person\" + 0.130*\"world\" '\n",
    "#>   '+ 0.124*\"war\"'),\n",
    "#>  (1, '0.937*\"agave\" + 0.164*\"asia\" + 0.100*\"aruba\" + 0.063*\"plant\" + 0.053*\"var\" '\n",
    "#>   '+ 0.052*\"state\" + 0.045*\"east\" + 0.044*\"congress\" + -0.042*\"first\" + '\n",
    "#>   '0.041*\"maguey\"'),\n",
    "#>  (2, '0.507*\"american\" + 0.180*\"football\" + 0.179*\"player\" + 0.168*\"war\" + '\n",
    "#>   '0.150*\"british\" + -0.140*\"also\" + 0.114*\"ball\" + 0.110*\"day\" + '\n",
    "#>   '-0.107*\"atheism\" + -0.106*\"god\"'),\n",
    "#>  (3, '-0.362*\"apollo\" + 0.248*\"lincoln\" + 0.211*\"state\" + -0.172*\"player\" + '\n",
    "#>   '-0.151*\"football\" + 0.127*\"union\" + -0.125*\"ball\" + 0.124*\"government\" + '\n",
    "#>   '-0.116*\"moon\" + 0.116*\"jews\"'),\n",
    "#>  (4, '-0.363*\"atheism\" + -0.334*\"god\" + -0.329*\"lincoln\" + -0.230*\"apollo\" + '\n",
    "#>   '-0.215*\"atheist\" + -0.143*\"abraham\" + 0.136*\"island\" + -0.132*\"aristotle\" + '\n",
    "#>   '0.124*\"aluminium\" + -0.119*\"belief\"'),\n",
    "#>  (5, '-0.360*\"apollo\" + 0.344*\"atheism\" + -0.326*\"lincoln\" + 0.226*\"god\" + '\n",
    "#>   '0.205*\"atheist\" + 0.139*\"american\" + -0.130*\"lunar\" + 0.128*\"football\" + '\n",
    "#>   '-0.125*\"moon\" + 0.114*\"belief\"'),\n",
    "#>  (6, '-0.313*\"lincoln\" + 0.226*\"apollo\" + -0.166*\"football\" + -0.163*\"war\" + '\n",
    "#>   '0.162*\"god\" + 0.153*\"australia\" + -0.148*\"play\" + -0.146*\"ball\" + '\n",
    "#>   '0.122*\"atheism\" + -0.122*\"line\"')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
